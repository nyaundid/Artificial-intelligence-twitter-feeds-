{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tweet_author_classification.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"ffzspIikHHSj","colab_type":"code","outputId":"e0288a37-e3d1-4027-b1ff-33a829b21f25","executionInfo":{"status":"ok","timestamp":1544031725479,"user_tz":360,"elapsed":61135,"user":{"displayName":"Tony Blonigan","photoUrl":"https://lh6.googleusercontent.com/-0bAvOi78-GM/AAAAAAAAAAI/AAAAAAAAAII/-NbZNQqNty0/s64/photo.jpg","userId":"08023802111302271315"}},"colab":{"base_uri":"https://localhost:8080/","height":853}},"cell_type":"code","source":["# define args\n","run_env = 'windows'\n","drop_user = True\n","simplify_http = True\n","remove_stop_words = False\n","apply_porter_stemmer = True\n","padding = 'post'\n","verbose = 1\n","n_samples = 150000\n","\n","!pip install gensim\n","from google.colab import drive\n","drive.mount('/content/drive')\n","wrk_dr = '/content/drive/My Drive/Colab Notebooks/'\n","\n","data_dir = wrk_dr + 'data/'\n","\n","import ast\n","import datetime\n","from gensim.models import Word2Vec\n","import io\n","from keras import backend as K\n","from keras import regularizers\n","from keras.layers import LSTM, GRU, GRUCell, Dense, Flatten, TimeDistributed, Dropout\n","from keras.layers.embeddings import Embedding\n","from keras.models import load_model, Model\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords\n","from nltk.stem import porter\n","#from nltk.tokenize import word_tokenize\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, f1_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","import string\n","\n","# load raw data\n","start = datetime.datetime.now()\n","twcs = pd.read_csv(data_dir + 'twcs.zip')[['author_id','text']]\n","print('upload time: ' + str(datetime.datetime.now() - start))\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting gensim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n","\u001b[K    100% |████████████████████████████████| 23.6MB 1.5MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n","Collecting smart-open>=1.2.1 (from gensim)\n","  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n","Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n","\u001b[K    100% |████████████████████████████████| 1.4MB 15.5MB/s \n","\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n","  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n","Collecting boto3 (from smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/e4/78a78e42a98ca242ec4ef820b07d8e951f96bd7cc31f67ce11a93e0909f6/boto3-1.9.59-py2.py3-none-any.whl (128kB)\n","\u001b[K    100% |████████████████████████████████| 133kB 28.9MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.11.29)\n","Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n","  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n","Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 22.2MB/s \n","\u001b[?25hCollecting botocore<1.13.0,>=1.12.59 (from boto3->smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/cf/71dfc14692883aaf709bc1098a56770173a760a14b0b1cb74471609181be/botocore-1.12.59-py2.py3-none-any.whl (5.1MB)\n","\u001b[K    100% |████████████████████████████████| 5.1MB 6.8MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.59->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n","Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.59->boto3->smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n","\u001b[K    100% |████████████████████████████████| 552kB 25.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n","  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n","  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n","Successfully built smart-open bz2file\n","Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n","Successfully installed boto-2.49.0 boto3-1.9.59 botocore-1.12.59 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["upload time: 0:00:19.432691\n"],"name":"stdout"}]},{"metadata":{"id":"FD6pKzowG6y8","colab_type":"code","outputId":"25f4d448-6a53-490a-d78c-7a2fb427f4ab","executionInfo":{"status":"ok","timestamp":1544031865836,"user_tz":360,"elapsed":87344,"user":{"displayName":"Tony Blonigan","photoUrl":"https://lh6.googleusercontent.com/-0bAvOi78-GM/AAAAAAAAAAI/AAAAAAAAAII/-NbZNQqNty0/s64/photo.jpg","userId":"08023802111302271315"}},"colab":{"base_uri":"https://localhost:8080/","height":918}},"cell_type":"code","source":["\n","# setup train and test size\n","if run_env == 'windows':\n","    val_and_test_prop = 0.07\n","    val_and_test_size = int(n_samples * val_and_test_prop)\n","    n_samples += val_and_test_size * 2\n","    twcs = twcs.sample(n=n_samples, random_state=345)\n","\n","# label tweet as one of 109 corp accounts, or non-corp acct (92 classes) 40% of data is other class\n","# consolidate non-corporate accounts\n","start = datetime.datetime.now()\n","twcs['author_id'] = twcs['author_id']\\\n","  .apply(lambda x: x if x.replace('_','')\\\n","  .replace('O2','O').isalpha() else 'non-corporate')\n","print('consolidate author time: ' + str(datetime.datetime.now() - start))\n","\n","# avoid not having class representitives in train and validate sets\n","if run_env == 'windows':\n","    n_tweets_by_author = twcs['author_id'].value_counts()\n","    twcs = twcs[twcs['author_id'].isin(n_tweets_by_author.index[n_tweets_by_author >= 500])]\n","    min_tweets = 500\n","    print(str(len(n_tweets_by_author[n_tweets_by_author >= min_tweets])) + ' included classes:')\n","    print(n_tweets_by_author[n_tweets_by_author >= min_tweets])\n","    val_and_test_size = int(len(twcs) * val_and_test_prop)\n","\n","# cleanse text\n","# remove @____ text\n","\n","drop_user_pattern = re.compile(\"(@[A-Za-z0-9]+)\")\n","http_pattern = re.compile('http[^\\s]+')\n","translator = str.maketrans('', '', string.punctuation)\n","porter_stemmer = porter.PorterStemmer()\n","\n","def word_cleanse(word):\n","    # porter stem & remove punctuation\n","    word = porter_stemmer.stem(word.translate(translator))\n","\n","    # simplify web addresses\n","    if simplify_http:\n","        return re.sub(pattern=http_pattern, repl='http', string=word)\n","    else:\n","        return word\n","\n","def text_cleanse(tweet_text):\n","    \"\"\"\n","    cleanse text column of the tweet\n","    :param tweet_text: string of words (tweet)\n","    :return: list of cleansed words in tweet text\n","    \"\"\"\n","    # replace @username tags with marker\n","    if drop_user:\n","        tweet_text = ' '.join(re.sub(drop_user_pattern, \" \", tweet_text).split())\\\n","            .lower()\\\n","            .replace('  ', ' ')\\\n","            .split(' ')\n","    else:\n","        tweet_text = tweet_text.lower() \\\n","            .replace('  ', ' ') \\\n","            .split(' ')\n","\n","    # remove stop words (decided not to do this because removes important words like et (eastern time))\n","    if remove_stop_words:\n","        tweet_text = [word for word in tweet_text.split(' ')\n","                      if (word not in stopwords.words('english'))\n","                      & (len(word)>2) & (len(word)<15)]\n","\n","    # apply porter stemmer & remove punctuation\n","    if apply_porter_stemmer:\n","        tweet_text = [word_cleanse(word=word) for word in tweet_text]\n","\n","    return tweet_text\n","\n","start = datetime.datetime.now()\n","twcs['text'] = twcs['text'].apply(lambda x: text_cleanse(tweet_text=x))\n","print('cleanse time: ' + str(datetime.datetime.now() - start))\n","\n","if run_env == 'colaboratory':\n","    twcs.to_csv(data_dir + 'twcs_cleansed.gzip', compression='gzip', index=False)\n","\n","twcs.head()\n","\n","if run_env == 'colaboratory':\n","    start = datetime.datetime.now()\n","    twcs = pd.read_csv(data_dir + 'twcs_cleansed.gzip', compression='gzip')\n","    print('read data time:' + str(datetime.datetime.now() - start))\n","\n","    start = datetime.datetime.now()\n","    twcs['text'] = twcs['text'].apply(ast.literal_eval)\n","    print('string to list time:' + str(datetime.datetime.now() - start))\n","\n","start = datetime.datetime.now()\n","x_train, x_val, y_train, y_val = \\\n","    train_test_split(twcs['text'], twcs['author_id'],\n","                     test_size=val_and_test_size,\n","                     stratify=twcs['author_id'],\n","                     random_state=3135,\n","                     shuffle=True)\n","\n","x_train, x_test, y_train, y_test = \\\n","    train_test_split(x_train, y_train,\n","                     test_size=val_and_test_size,\n","                     stratify=y_train,\n","                     random_state=3135,\n","                     shuffle=True)\n","\n","x_train.reset_index(inplace=True, drop=True)\n","x_val.reset_index(inplace=True, drop=True)\n","x_test.reset_index(inplace=True, drop=True)\n","print('split time: ' + str(datetime.datetime.now() - start))\n","\n","# reformat y to one-hot encoding for Keras cat target\n","y_encoder = LabelBinarizer().fit(y_val.values)\n","y_train = y_encoder.transform(y_train)\n","y_val = y_encoder.transform(y_val)\n","y_test = y_encoder.transform(y_test)\n","\n","def to_unique_words(seq, idfun=None):\n","   # order preserving\n","   if idfun is None:\n","       def idfun(x): return x\n","   seen = {}\n","   result = []\n","   for sent in seq:\n","       for item in sent:\n","           marker = idfun(item)\n","           if marker in seen: continue\n","           seen[marker] = 1\n","           result.append(item)\n","   return result\n","\n","\n","unique_words = to_unique_words(x_train)\n","vocab_size = len(unique_words)\n","\n","# convert x to sequence data\n","sent_len = x_train.apply(len)\n","max_sent_len = np.max(sent_len)\n","print('\\nmax_steps: ' + str(max_sent_len))\n","\n","\n","def to_token(x, vocab_size, max_len, padding):\n","    x = x.apply(lambda s: ' '.join(s))\n","    x = [one_hot(w, vocab_size) for w in x]\n","    return pad_sequences(x, maxlen=max_len, padding=padding)\n","\n","start = datetime.datetime.now()\n","x_train = to_token(x=x_train, vocab_size=vocab_size, max_len=max_sent_len, padding=padding)\n","x_val = to_token(x=x_val, vocab_size=vocab_size, max_len=max_sent_len, padding=padding)\n","x_test = to_token(x=x_test, vocab_size=vocab_size, max_len=max_sent_len, padding=padding)\n","print('to_token time:' + str(datetime.datetime.now() - start))\n","\n","def plot_training_results(metric, history, nn, x_test, y_test, file=None):\n","    test_loss, test_acc = nn.evaluate(x=x_test, y=y_test)\n","    test_result = test_loss if metric == 'loss' else test_acc\n","\n","    plt.figure(figsize=(8*1.5, 6*1.5))\n","    plt.plot(history.history[metric], label='train')\n","    plt.plot(history.history['val_' + metric], label='val')\n","    plt.legend()\n","    plt.title(metric.title() + ' by Epoch    |    Test ' + metric.title() + ': ' + str(round(test_result, 3)))\n","\n","    if file is not None:\n","        plt.savefig(file, transparent=True)\n","    else:\n","        plt.show()\n","\n","    plt.close()\n","\n","# y = y_test\n","# x=x_test\n","# nn=model\n","def plot_confusion_matrix(x, y, nn, file=None):\n","    y = np.argmax(y, axis=1)\n","    y_hat = nn.predict(x=x)\n","    y_hat = np.argmax(y_hat, axis=1)\n","\n","    f1 = np.round(f1_score(y, y_hat, average='micro'), 3)\n","\n","    conf_matrix = np.log(1 + confusion_matrix(y_true=y, y_pred=y_hat))\n","    # conf_matrix = confusion_matrix(y_true=y, y_pred=y_hat)\n","\n","    plt.figure(figsize=(8 * 1.5, 6 * 1.5))\n","    sns.heatmap(conf_matrix, center=np.median(conf_matrix))\n","    plt.title('Confusion Matrix (log of 1 + count)    |    F1: ' + str(f1))\n","\n","    if file is not None:\n","        plt.savefig(file, transparent=True)\n","    else:\n","        plt.show()\n","\n","    plt.close()\n","\n","# plot_confusion_matrix(x=x_test, y=y_test, nn=model)\n","\n","# # # define fully connected model\n","# fc_model = Sequential()\n","# fc_model.add(Embedding(vocab_size, 500, input_length=max_sent_len))\n","# fc_model.add(Flatten())\n","# fc_model.add(Dense(200, activation='relu'))\n","# fc_model.add(Dropout(.3))\n","# fc_model.add(Dense(100, activation='relu'))\n","# fc_model.add(Dropout(.3))\n","# fc_model.add(Dense(50, activation='relu'))\n","# fc_model.add(Dense(y_val.shape[1], activation='softmax'))\n","# # compile the model\n","# fc_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","# # summarize the model\n","# print(fc_model.summary())\n","# # fit the model\n","# start = datetime.datetime.now()\n","# history = fc_model.fit(x_train, y_train, epochs=5, verbose=verbose, validation_data=[x_val, y_val], batch_size=256)\n","# print('fc train time: ' + str(datetime.datetime.now() - start))\n","#\n","# plot_training_results(metric='acc', history=history, nn=fc_model, x_test=x_test, y_test=y_test,\n","#                       file=data_dir + 'fc_acc.png')\n","# plot_training_results(metric='loss', history=history, nn=fc_model, x_test=x_test, y_test=y_test,\n","#                       file=data_dir + 'fc_loss.png')\n","# plot_confusion_matrix(x=x_test, y=y_test, nn=fc_model, file=data_dir + 'fc_confusion.png')\n","\n","# t = np.bincount(np.argmax(y_test, axis=1)) / np.sum(np.argmax(y_test, axis=1))\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["consolidate author time: 0:00:00.158845\n","45 included classes:\n","non-corporate      93591\n","AmazonHelp         10169\n","AppleSupport        6486\n","Uber_Support        3422\n","SpotifyCares        2730\n","Delta               2488\n","Tesco               2342\n","AmericanAir         2287\n","TMobileHelp         2162\n","comcastcares        2077\n","SouthwestAir        1755\n","British_Airways     1746\n","VirginTrains        1665\n","Ask_Spectrum        1556\n","XboxSupport         1512\n","sprintcare          1336\n","hulu_support        1315\n","sainsburys          1205\n","GWRHelp             1182\n","AskPlayStation      1149\n","VerizonSupport      1140\n","ChipotleTweets      1116\n","UPSHelp             1090\n","ATVIAssist          1039\n","Safaricom_Care       982\n","idea_cares           958\n","O2                   930\n","AskTarget            869\n","AirAsiaSupport       787\n","SW_Help              739\n","ArgosHelpers         726\n","BofA_Help            726\n","AskPayPal            720\n","AskLyft              704\n","MicrosoftHelps       671\n","AskAmex              670\n","marksandspencer      667\n","AdobeCare            639\n","airtel_care          599\n","AskeBay              569\n","Morrisons            548\n","McDonalds            539\n","AirbnbHelp           526\n","JetBlue              518\n","ChaseSupport         516\n","Name: author_id, dtype: int64\n","cleanse time: 0:01:18.487087\n","split time: 0:00:00.424668\n","\n","max_steps: 68\n","to_token time:0:00:05.316172\n"],"name":"stdout"}]},{"metadata":{"id":"VCkX2aYs5CZo","colab_type":"code","outputId":"d5792eb4-18f7-4e1e-d776-12a2a62cca0e","executionInfo":{"status":"ok","timestamp":1544031878802,"user_tz":360,"elapsed":94485,"user":{"displayName":"Tony Blonigan","photoUrl":"https://lh6.googleusercontent.com/-0bAvOi78-GM/AAAAAAAAAAI/AAAAAAAAAII/-NbZNQqNty0/s64/photo.jpg","userId":"08023802111302271315"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"cell_type":"code","source":["from keras.models import load_model\n","\n","# load model\n","fc_model = load_model(data_dir + 'fc_model.h5')\n","\n","fc_model.summary()\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 68, 500)           31679000  \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 34000)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 200)               6800200   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 200)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 100)               20100     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 100)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 50)                5050      \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 45)                2295      \n","=================================================================\n","Total params: 38,506,645\n","Trainable params: 38,506,645\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"rnO8JlnL7E-s","colab_type":"code","outputId":"8fbe0fbf-3675-4a1a-950c-18d024761c28","executionInfo":{"status":"ok","timestamp":1544034217893,"user_tz":360,"elapsed":2345758,"user":{"displayName":"Tony Blonigan","photoUrl":"https://lh6.googleusercontent.com/-0bAvOi78-GM/AAAAAAAAAAI/AAAAAAAAAII/-NbZNQqNty0/s64/photo.jpg","userId":"08023802111302271315"}},"colab":{"base_uri":"https://localhost:8080/","height":646}},"cell_type":"code","source":["# take feature extractor\n","base_model = Model(inputs=fc_model.input, \n","              outputs=fc_model.get_layer('embedding_1').output)\n","\n","# load model\n","# input = Input(batch_shape=(5, 224, 224, 3))\n","# base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\"\"\"\n","x = base_model.output\n","x = Flatten()(x)\n","x = Dense(32)(x)\n","x = Activation('relu')(x)\n","x = Dense(16)(x)\n","x = Activation('relu')(x)\n","x = Dense(3)(x)\n","predictions = Activation('softmax')(x)\n","\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# freeze vgg19 layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# compile model\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","\"\"\"\n","\n","# extend the feature extractor\n","# define sequential model\n","batch_size = 2**9\n","embed_size = 500\n","epochs = 4\n","lstm_model = base_model.output\n","\n","#lstm_model = Sequential()\n","# lstm_model = LSTM(200, return_sequences=True, stateful=False)(lstm_model)\n","lstm_model = GRU(200, return_sequences=False, stateful=False)(lstm_model)\n","lstm_model = Dropout(.4)(lstm_model)\n","lstm_model = Dense(100)(lstm_model)\n","lstm_model = Dropout(.2)(lstm_model)\n","lstm_model = Dense(50)(lstm_model)\n","# lstm_model = Dropout(.2)(lstm_model)\n","# lstm_model = Dense(50)(lstm_model)\n","predictions = Dense(y_val.shape[1], activation='softmax')(lstm_model)\n","\n","full_lstm_model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# freeze embedding layer\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# compile the model\n","full_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","# summarize the model\n","print(full_lstm_model.summary())\n","\n","# fit the model\n","loss = []\n","val_loss = []\n","acc = []\n","val_acc = []\n","\n","train_minutes = 1\n","start = datetime.datetime.now()\n","while datetime.datetime.now() - start < datetime.timedelta(minutes=train_minutes):\n","    history = full_lstm_model.fit(x=x_train, y=y_train, \n","                                  validation_data=[x_val, y_val], \n","                                  epochs=epochs, batch_size=batch_size, \n","                                  shuffle=True, verbose=verbose)\n","\n","    loss.append(history.history['loss'])\n","    val_loss.append(history.history['val_loss'])\n","    acc.append(history.history['acc'])\n","    val_acc.append(history.history['val_acc'])\n","\n","history.history['loss'] = np.array(loss).ravel()\n","history.history['val_loss'] = np.array(val_loss).ravel()\n","history.history['acc'] = np.array(acc).ravel()\n","history.history['val_acc'] = np.array(val_acc).ravel()\n","\n","print(history.history['loss'])\n","print(history.history['val_loss'])\n","\n","plot_training_results(metric='loss', history=history, nn=full_lstm_model, x_test=x_test, y_test=y_test,\n","                      file=data_dir + 'lstm_loss.png')\n","plot_training_results(metric='acc', history=history, nn=full_lstm_model, x_test=x_test, y_test=y_test,\n","                      file=data_dir + 'lstm_acc_loss.png')\n","plot_confusion_matrix(x=x_test, y=y_test, nn=full_lstm_model, file=data_dir + 'lstm_confusion.png')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1_input (InputLaye (None, 68)                0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 68, 500)           31679000  \n","_________________________________________________________________\n","gru_1 (GRU)                  (None, 200)               420600    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 200)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 100)               20100     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 100)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 50)                5050      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 45)                2295      \n","=================================================================\n","Total params: 32,127,045\n","Trainable params: 448,045\n","Non-trainable params: 31,679,000\n","_________________________________________________________________\n","None\n","Train on 138601 samples, validate on 11281 samples\n","Epoch 1/4\n","138601/138601 [==============================] - 565s 4ms/step - loss: 2.1901 - acc: 0.5788 - val_loss: 2.1230 - val_acc: 0.5807\n","Epoch 2/4\n","138601/138601 [==============================] - 564s 4ms/step - loss: 2.1320 - acc: 0.5807 - val_loss: 2.1228 - val_acc: 0.5807\n","Epoch 3/4\n","138601/138601 [==============================] - 564s 4ms/step - loss: 2.1286 - acc: 0.5807 - val_loss: 2.1195 - val_acc: 0.5807\n","Epoch 4/4\n","138601/138601 [==============================] - 570s 4ms/step - loss: 2.1264 - acc: 0.5807 - val_loss: 2.1203 - val_acc: 0.5807\n","[2.19010598 2.1320346  2.12855032 2.12637581]\n","[2.12304034 2.12276963 2.11954842 2.12028182]\n","11281/11281 [==============================] - 25s 2ms/step\n","11281/11281 [==============================] - 25s 2ms/step\n"],"name":"stdout"}]},{"metadata":{"id":"NZb5JJ63757j","colab_type":"code","outputId":"ab9bea66-bfcf-424f-9c10-5d0acc1cefaf","executionInfo":{"status":"ok","timestamp":1544037896525,"user_tz":360,"elapsed":75018,"user":{"displayName":"Tony Blonigan","photoUrl":"https://lh6.googleusercontent.com/-0bAvOi78-GM/AAAAAAAAAAI/AAAAAAAAAII/-NbZNQqNty0/s64/photo.jpg","userId":"08023802111302271315"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["plot_training_results(metric='loss', history=history, nn=full_lstm_model, x_test=x_test, y_test=y_test,\n","                      file=data_dir + 'lstm_loss.png')\n","plot_training_results(metric='acc', history=history, nn=full_lstm_model, x_test=x_test, y_test=y_test,\n","                      file=data_dir + 'lstm_acc_loss.png')\n","plot_confusion_matrix(x=x_test, y=y_test, nn=full_lstm_model, file=data_dir + 'lstm_confusion.png')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["11281/11281 [==============================] - 26s 2ms/step\n","11281/11281 [==============================] - 25s 2ms/step\n"],"name":"stdout"}]},{"metadata":{"id":"zW4qu0q0LDW5","colab_type":"code","outputId":"63c6e496-9a53-4078-9f73-139cbc55b9f3","executionInfo":{"status":"error","timestamp":1543771215659,"user_tz":360,"elapsed":5097,"user":{"displayName":"Tony Blonigan","photoUrl":"https://lh6.googleusercontent.com/-0bAvOi78-GM/AAAAAAAAAAI/AAAAAAAAAII/-NbZNQqNty0/s64/photo.jpg","userId":"08023802111302271315"}},"colab":{"base_uri":"https://localhost:8080/","height":1542}},"cell_type":"code","source":["# define sequential model\n","batch_size = 2**9\n","embed_size = 500\n","epochs = 4\n","lstm_model = Sequential()\n","lstm_model.add(Embedding(vocab_size, embed_size, input_length=max_sent_len))\n","# lstm_model.add(LSTM(200, return_sequences=True, stateful=False))\n","lstm_model.add(LSTM(500, return_sequences=False, stateful=False))\n","lstm_model.add(Dropout(.4))\n","lstm_model.add(Dense(200))\n","lstm_model.add(Dropout(.2))\n","lstm_model.add(Dense(50))\n","lstm_model.add(Dropout(.2))\n","lstm_model.add(Dense(50))\n","lstm_model.add(Dense(y_val.shape[1], activation='softmax'))\n","# compile the model\n","lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","# summarize the model\n","print(lstm_model.summary())\n","# fit the model\n","loss = []\n","val_loss = []\n","acc = []\n","val_acc = []\n","\n","train_minutes = 1\n","start = datetime.datetime.now()\n","while datetime.datetime.now() - start < datetime.timedelta(minutes=train_minutes):\n","    history = lstm_model.fit(x=x_train, y=y_train, validation_data=[x_val, y_val],\n","                             epochs=epochs, batch_size=batch_size, shuffle=True, verbose=verbose)\n","\n","    loss.append(history.history['loss'])\n","    val_loss.append(history.history['val_loss'])\n","    acc.append(history.history['acc'])\n","    val_acc.append(history.history['val_acc'])\n","\n","history.history['loss'] = np.array(loss).ravel()\n","history.history['val_loss'] = np.array(val_loss).ravel()\n","history.history['acc'] = np.array(acc).ravel()\n","history.history['val_acc'] = np.array(val_acc).ravel()\n","\n","print(history.history['loss'])\n","print(history.history['val_loss'])\n","\n","plot_training_results(metric='loss', history=history, nn=lstm_model, x_test=x_test, y_test=y_test,\n","                      file=data_dir + 'lstm_loss.png')\n","plot_training_results(metric='acc', history=history, nn=lstm_model, x_test=x_test, y_test=y_test,\n","                      file=data_dir + 'lstm_acc_loss.png')\n","plot_confusion_matrix(x=x_test, y=y_test, nn=lstm_model, file=data_dir + 'lstm_confusion.png')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 68, 500)           31679000  \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 500)               2002000   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 500)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 200)               100200    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 200)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 50)                10050     \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 50)                0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 50)                2550      \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 45)                2295      \n","=================================================================\n","Total params: 33,796,095\n","Trainable params: 33,796,095\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Train on 138601 samples, validate on 11281 samples\n","Epoch 1/4\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-119db1ba2ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminutes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_minutes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     history = lstm_model.fit(x=x_train, y=y_train, validation_data=[x_val, y_val],\n\u001b[0;32m---> 29\u001b[0;31m                              epochs=epochs, batch_size=batch_size, shuffle=True, verbose=verbose)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}